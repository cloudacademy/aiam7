{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AAhf5MqnS9oG"
   },
   "source": [
    "# Improving Agentic Systems with RAG and Prompt Engineering Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we'll explore how to use prompt engineering and RAG to improve agentic system performance. First use the open-source model Zephyr-7b beta (a fine tuned version of Mistral-7B, developed by mistral.ai) to observe how changes in prompts influence the model's output. Then, we'll build a RAG system from scratch to help our system provide correct responses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AC5KmEpIyr0a"
   },
   "source": [
    "Below, we filter out any annoying warning messages that might pop up when using certain libraries. Your code will run the same without doing this, we just prefer a cleaner output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vgcmH5bSM-x8"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vi2x6zvgbhLO"
   },
   "source": [
    "# Part 1: Prompt Engineering - Designing a Prompt for Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll follow the below workflow we saw in module 2 to install apppropriate packages and instantiate our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JUPQJKRwfF79"
   },
   "source": [
    "## 🔧 Step 1: Install Required Packages\n",
    "Install `transformers` and `accelerate`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EeJ8jRVzygmE",
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install -U transformers \n",
    "!pip install -U accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2UV7KvIT1O8G"
   },
   "source": [
    "## 🤖 Step 2: Import Required Libraries\n",
    "Import from `transformers`:\n",
    "* `AutoModelForCausalLM`\n",
    "* `AutoTokenizer`\n",
    "* `pipeline`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u9RnBxBC2TCb"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "305pOtCGDCc9"
   },
   "source": [
    "## 🔄 Step 3: Load the Tokenizer\n",
    "Use `AutoTokenizer` to load the tokenozer for `HuggingFaceH4/zephyr-7b-beta`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jf-Jrl4MzBkh"
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceH4/zephyr-7b-beta\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YfbMlQAk9Vf1"
   },
   "source": [
    "## 🧠 Step 4: Load the model - Zephyr-7b-Beta (a version of Mistral 7B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T9w4oQT1-AC-"
   },
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"HuggingFaceH4/zephyr-7b-beta\",\n",
    "    device_map=\"cuda\",\n",
    "    torch_dtype=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dq_olWhO1TRG"
   },
   "source": [
    "## 🗞 Step 5: Wrap everything in a pipeline object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qw6iC5JKzm_z"
   },
   "outputs": [],
   "source": [
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    return_full_text=False,\n",
    "    max_new_tokens=500,\n",
    "    do_sample=False,\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Engineering a Prompt for Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cY5xuxMqgf1O"
   },
   "source": [
    "## 🌟 Activity\n",
    "Create a prompt that guides a text generation model to classify a piece of text into categories related to food. \n",
    "\n",
    "For example, classify text into categories like 'Cuisine Type,' 'Cooking Techniques,' 'Ingredients,' or 'Food Culture.' Define the persona, task, and expected output format in your prompt. Use the following sample text to test your prompt:\n",
    "\n",
    "Sample Text: \"Spaghetti carbonara is a classic Italian pasta dish made with eggs, cheese, pancetta, and pepper. It originated in the Lazio region of Italy and is known for its creamy texture, achieved without using cream. The dish is quick to prepare and relies on high-quality ingredients for the best flavor.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fmmQmkgDjZQm"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rSfkirIYjZte"
   },
   "source": [
    "## Chain of Thought Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RfFhVFWRc7IZ"
   },
   "source": [
    "## 🌟 Activity\n",
    "\n",
    "Get the model to try and solve the following questions, first without COT, and then using COT:\n",
    "\n",
    "1. Lily has twice as many apples as John. Together, they have 36 apples. How many apples does each of them have?\n",
    "2. There are five houses in a row, each painted a different color. In each house lives a person with a different profession. The teacher lives in the red house. The doctor lives next to the teacher. The engineer lives two houses away from the teacher. Who lives in the blue house?\n",
    "\n",
    "Ensure the model generates the correct output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Implementing a RAG System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#x1F6A7; **You will need to restart the notebook kernel before starting this part of the demo otherwise you will run out of GPU memory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xvps7UpznPD4",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AC5KmEpIyr0a"
   },
   "source": [
    "Below, we filter out any annoying warning messages that might pop up when using certain libraries. Your code will run the same without doing this, we just prefer a cleaner output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vgcmH5bSM-x8",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll follow the below workflow to install apppropriate packages and instantiate our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JUPQJKRwfF79"
   },
   "source": [
    "## 🔧 Step 1: Install Required Packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_dRaJxqJNfM0",
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install langchain \n",
    "!pip install langgraph\n",
    "!pip install langchain-huggingface \n",
    "!pip install -U langchain-community\n",
    "!pip install faiss-gpu \n",
    "!pip install transformers \n",
    "!pip install accelerate \n",
    "!pip install datasets \n",
    "!pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2UV7KvIT1O8G"
   },
   "source": [
    "## 🤖 Step 2: Import Required Libraries\n",
    "Complete the below imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers \n",
    "from langchain_huggingface \n",
    "from langchain.text_splitter \n",
    "from langchain.schema\n",
    "from langchain.vectorstores \n",
    "from langchain_huggingface\n",
    "from datasets \n",
    "from langgraph.graph \n",
    "from langchain.schema.runnable\n",
    "from typing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "305pOtCGDCc9",
    "tags": []
   },
   "source": [
    "## 🔄 Step 3: Load the Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the tokenizer for `microsoft/phi-3-mini-4k-instruct`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YfbMlQAk9Vf1"
   },
   "source": [
    "## 🧠 Step 4: Load the model - Phi 3 Mini 4k Instruct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the model for `microsoft/phi-3-mini-4k-instruct`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dq_olWhO1TRG"
   },
   "source": [
    "## 🗞 Step 5: Wrap everything in a pipeline object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## &#x1F4F0; Step 6: Process the Documents that will form the Knowledge Base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `sentence-transformers/natural-questions` dataset from Huggingface (originally Google, info here https://ai.google.com/research/NaturalQuestions) to create a list of documents called `docs`, then use the `CharacterTextSplitter` to split the documents into chunks of size 500 with a 50 character overlap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## &#x1F522; Step 7: Embed the Documents in the Vector Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `all-MiniLM-L6-v2` model to embed the chunks of text and create a retreiver to query the vectorstore, setting the number of documents to retrieve as 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## &#x1F531; Step 8: Define Graph Components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a `RAGState` class so it expects a question, possibly some helpful documents and eventually an answer.\n",
    "\n",
    "Then define two functions representing our nodes: \n",
    "* `retrieve` , which gets the current question from the state, searches for documents similar to the question in the vectorstore, and then updates the state so that docs points to the relevant documents we found.\n",
    "* `generate` , which gets the current question and docs from the state, constructs a prompt so that it contains the relevant documents that were found, the question, and a slot for the answer, then pass the prompt to our language model. Update the state so that answer contains the response from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## &#x25B6; Step 9: Build Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct a graph where the flow of events are retrieve -> generate -> answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WnhqZpXv1eur"
   },
   "source": [
    "## 🧪 Step 10: Test the System by Asking a Question it Should Know About\n",
    "Try the system with the question \"What do the red stripes mean on the american flag?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ✅❌ Step 11: Evaluate System Performance\n",
    "Take the evaluation code from the demo and see if it works for the new dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6vbYhZiVb05j"
   },
   "source": [
    "## Stretch activities 🌟\n",
    "\n",
    "Now that you’ve built a system where an LLMs responses are grounded by a RAG system, here are some suggested activities to build your knowledge:\n",
    "\n",
    "1. Swap the open-source HuggingFace LLM for a GPT 3.5 agent. You'll need to adapt the model loading code and the functions that define the nodes of the graph.\n",
    "2. Build the RAG system into a multi agent swarm.\n",
    "3. Figure out a better way of evaluating the response of LLM with respect to the ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
