{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AAhf5MqnS9oG"
   },
   "source": [
    "# Improving Agentic Systems with RAG and Prompt Engineering Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we'll explore how to use prompt engineering and RAG to improve agentic system performance. First use the open-source model Zephyr-7b beta (a fine tuned version of Mistral-7B, developed by mistral.ai) to observe how changes in prompts influence the model's output. Then, we'll build a RAG system from scratch to help our system provide correct responses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AC5KmEpIyr0a"
   },
   "source": [
    "Below, we filter out any annoying warning messages that might pop up when using certain libraries. Your code will run the same without doing this, we just prefer a cleaner output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "vgcmH5bSM-x8"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vi2x6zvgbhLO"
   },
   "source": [
    "# Part 1: Prompt Engineering - Designing a Prompt for Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll follow the below workflow we saw in module 2 to install apppropriate packages and instantiate our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JUPQJKRwfF79"
   },
   "source": [
    "## 🔧 Step 1: Install Required Packages\n",
    "Install `transformers` and `accelerate`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "EeJ8jRVzygmE",
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install -U transformers \n",
    "!pip install -U accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2UV7KvIT1O8G"
   },
   "source": [
    "## 🤖 Step 2: Import Required Libraries\n",
    "Import from `transformers`:\n",
    "* `AutoModelForCausalLM`\n",
    "* `AutoTokenizer`\n",
    "* `pipeline`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "u9RnBxBC2TCb"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "305pOtCGDCc9"
   },
   "source": [
    "## 🔄 Step 3: Load the Tokenizer\n",
    "Use `AutoTokenizer` to load the tokenozer for `HuggingFaceH4/zephyr-7b-beta`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Jf-Jrl4MzBkh"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22359f4a8fa64fd7a9e79d7218807f5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efe14f6aa2d24ef39bcf37c9d2f61169",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89a5122f960a4014b10581a7b7df9cf4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f89baabc40f44febb27f3d804a31f9de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/42.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0271d86823e64de9886a4ee9661e35c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/168 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceH4/zephyr-7b-beta\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YfbMlQAk9Vf1"
   },
   "source": [
    "## 🧠 Step 4: Load the model - Zephyr-7b-Beta (a version of Mistral 7B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "T9w4oQT1-AC-"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0b9042d3c4444279a16dbc205998c52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/638 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3381149508334b839852b478bdba4965",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60cd1bd9f0324b80998133b2661a7c96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 8 files:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f80d5de92254df587964ac6cdaf73c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00008.safetensors:   0%|          | 0.00/1.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05ffc8eb920e4c08b7fc75ae5bb29ff5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00008-of-00008.safetensors:   0%|          | 0.00/816M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d06c458055944e59f5e5c0c427b5948",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00008.safetensors:   0%|          | 0.00/1.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e28edca0c8b416f9d74debbedcf2717",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00008.safetensors:   0%|          | 0.00/1.89G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f0af06f60534c97aab170855ab6d92a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00006-of-00008.safetensors:   0%|          | 0.00/1.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b59cd4a25fcf46d491139cf815769303",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00005-of-00008.safetensors:   0%|          | 0.00/1.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9770221df0a4b5ca98049d3d36c6278",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00007-of-00008.safetensors:   0%|          | 0.00/1.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ee6e57844ff4c40a6adb87a1c25dd20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00008.safetensors:   0%|          | 0.00/1.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04ea0c207f364931a2197af8a3e83bc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf929e1992f848a688d6f0b491c66c37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"HuggingFaceH4/zephyr-7b-beta\",\n",
    "    device_map=\"cuda\",\n",
    "    torch_dtype=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dq_olWhO1TRG"
   },
   "source": [
    "## 🗞 Step 5: Wrap everything in a pipeline object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Qw6iC5JKzm_z"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda\n",
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    return_full_text=False,\n",
    "    max_new_tokens=500,\n",
    "    do_sample=False,\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Engineering a Prompt for Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cY5xuxMqgf1O"
   },
   "source": [
    "## 🌟 Activity\n",
    "Create a prompt that guides a text generation model to classify a piece of text into categories related to food. \n",
    "\n",
    "For example, classify text into categories like 'Cuisine Type,' 'Cooking Techniques,' 'Ingredients,' or 'Food Culture.' Define the persona, task, and expected output format in your prompt. Use the following sample text to test your prompt:\n",
    "\n",
    "Sample Text: \"Spaghetti carbonara is a classic Italian pasta dish made with eggs, cheese, pancetta, and pepper. It originated in the Lazio region of Italy and is known for its creamy texture, achieved without using cream. The dish is quick to prepare and relies on high-quality ingredients for the best flavor.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "fmmQmkgDjZQm"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category: Cuisine Type\n",
      "\n",
      "Explanation: The text provides a brief description of spaghetti carbonara, a specific dish from Italian cuisine. It highlights the regional origin of the dish and its traditional ingredients, which are essential to its identity as a classic Italian pasta dish. Therefore, the main focus of the text is on the cuisine type, specifically the Italian dish of spaghetti carbonara.\n"
     ]
    }
   ],
   "source": [
    "# Prompt components\n",
    "persona = \"You are a food journalist who excels at identifying the main focus of food-related content.\\n\"\n",
    "instruction = \"Classify the provided text into one of the following categories: 'Cuisine Type,' 'Cooking Techniques,' 'Ingredients,' or 'Food Culture'.\\n\"\n",
    "context = \"Examine the content carefully to determine the main subject of the text and provide the most suitable category.\\n\"\n",
    "data_format = \"Respond with the category label and a brief explanation of your reasoning.\\n\"\n",
    "audience = \"This classification is intended for a general audience interested in understanding culinary writing.\\n\"\n",
    "tone = \"The response should be clear, concise, and easy to understand.\\n\"\n",
    "\n",
    "text = \"Spaghetti carbonara is a classic Italian pasta dish made with eggs, cheese, pancetta, and pepper. It originated in the Lazio region of Italy and is known for its creamy texture, achieved without using cream. The dish is quick to prepare and relies on high-quality ingredients for the best flavor.\"\n",
    "data = f\"Text to classify: {text}\"\n",
    "\n",
    "# The full prompt\n",
    "query = persona + instruction + context + data_format + audience + tone + data\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": query}\n",
    "]\n",
    "# Generate the output\n",
    "outputs = pipe(messages)\n",
    "print(outputs[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rSfkirIYjZte"
   },
   "source": [
    "## Chain of Thought Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RfFhVFWRc7IZ"
   },
   "source": [
    "## 🌟 Activity\n",
    "\n",
    "Get the model to try and solve the following questions, first without COT, and then using COT:\n",
    "\n",
    "1. Lily has twice as many apples as John. Together, they have 36 apples. How many apples does each of them have?\n",
    "2. There are five houses in a row, each painted a different color. In each house lives a person with a different profession. The teacher lives in the red house. The doctor lives next to the teacher. The engineer lives two houses away from the teacher. Who lives in the blue house?\n",
    "\n",
    "Ensure the model generates the correct output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_cot = (\n",
    "    \"Q: Lily has twice as many apples as John. Together, they have 36 apples. \"\n",
    "    \"How many apples does each of them have?\\n\"\n",
    "    \"Let's think step by step.\\nA:\"\n",
    ")\n",
    "\n",
    "print(pipe(prompt_cot)[0]['generated_text'])\n",
    "\n",
    "\n",
    "prompt_cot_logic = (\n",
    "    \"Q: There are five houses in a row, each painted a different color. In each house lives a person with a different profession. \"\n",
    "    \"The teacher lives in the red house. The doctor lives next to the teacher. The engineer lives two houses away from the teacher. \"\n",
    "    \"Who lives in the blue house?\\n\"\n",
    "    \"Let's think step by step.\\nA:\"\n",
    ")\n",
    "\n",
    "print(pipe(prompt_cot_logic)[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Implementing a RAG System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#x1F6A7; **You will need to restart the notebook kernel before starting this part of the demo otherwise you will run out of GPU memory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Xvps7UpznPD4",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AC5KmEpIyr0a"
   },
   "source": [
    "Below, we filter out any annoying warning messages that might pop up when using certain libraries. Your code will run the same without doing this, we just prefer a cleaner output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vgcmH5bSM-x8",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll follow the below workflow to install apppropriate packages and instantiate our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JUPQJKRwfF79"
   },
   "source": [
    "## 🔧 Step 1: Install Required Packages\n",
    "The new package being installed here is `faiss-gpu`, a GPU-accelerated version of FAISS, a library for efficient similarity search and clustering of dense vectors, which speeds up vector retrieval by running computations on the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_dRaJxqJNfM0",
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install langchain \n",
    "!pip install langgraph\n",
    "!pip install langchain-huggingface \n",
    "!pip install -U langchain-community\n",
    "!pip install faiss-gpu \n",
    "!pip install transformers \n",
    "!pip install accelerate \n",
    "!pip install datasets \n",
    "!pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2UV7KvIT1O8G"
   },
   "source": [
    "## 🤖 Step 2: Import Required Libraries\n",
    "The new imports are:\n",
    "* `from langchain_huggingface import HuggingFacePipeline`\n",
    "  Loads a Hugging Face model into a LangChain-compatible wrapper for LLM-based chains.\n",
    "* `from langchain.text_splitter import CharacterTextSplitter`\n",
    "  Splits large texts into smaller chunks based on character count, useful for document indexing.\n",
    "* `from langchain.schema import Document`\n",
    "  Defines the standard format for documents used in LangChain pipelines.\n",
    "* `from langchain.vectorstores import FAISS`\n",
    "  Provides access to the FAISS vector database for efficient similarity search and retrieval.\n",
    "* `from langchain_huggingface import HuggingFaceEmbeddings`\n",
    "  Loads Hugging Face embedding models for generating dense vector representations of text.\n",
    "* `from datasets import load_dataset`\n",
    "  Loads benchmark datasets (like SQuAD) from the Hugging Face datasets library.\n",
    "* `from langgraph.graph import StateGraph, END`\n",
    "  Constructs modular state-based execution graphs for chaining together AI agent components.\n",
    "* `from langchain.schema.runnable import RunnableLambda`\n",
    "  Wraps Python functions as LangChain-compatible steps in a processing chain.\n",
    "* `from typing import TypedDict, List, Optional`\n",
    "  Provides static type definitions for better structure and type-checking in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from datasets import load_dataset\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain.schema.runnable import RunnableLambda\n",
    "from typing import TypedDict, List, Optional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "305pOtCGDCc9",
    "tags": []
   },
   "source": [
    "## 🔄 Step 3: Load the Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"microsoft/phi-3-mini-4k-instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YfbMlQAk9Vf1"
   },
   "source": [
    "## 🧠 Step 4: Load the model - Phi 3 Mini 4k Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dq_olWhO1TRG"
   },
   "source": [
    "## 🗞 Step 5: Wrap everything in a pipeline object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=256, return_full_text=False)\n",
    "llm = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## &#x1F4F0; Step 6: Process the Documents that will form the Knowledge Base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we're loading the Stanford Question Answering Dataset (SQuAD), which is a reading comprehension dataset, consisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage, or the question might be unanswerable.\n",
    "\n",
    "We're using it as it contains lots of segments of text we can put into a vector store with corresponding questions we can ask to see if the RAG system works!\n",
    "\n",
    "We load in the dataset from HuggingFace, create a list of langchain Documents from the unique pieces of text, and then split the pieces of text (if larger than 500 characters) into overlapping chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "squad = load_dataset(\"squad\", split=\"validation\")\n",
    "docs = [Document(page_content=sample) for sample in squad.unique('context')]\n",
    "splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "split_docs = splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## &#x1F522; Step 7: Embed the Documents in the Vector Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we use the `all-MiniLM-L6-v2` model to embed the chunks of text i.e. encode them as vectors where \"closeness\" means similarity of meaning. These embeddings are then stored in a vectorstore with Facebook AI Similarity Search (FAISS), a library developed by Facebook for efficient similarity search and clustering of dense vectors.\n",
    "\n",
    "We set k = 3, where k is the number of similar documents we want to get back from a search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "vectorstore = FAISS.from_documents(split_docs, embedding_model)\n",
    "retriever = vectorstore.as_retriever(search_kwargs={'k': 3})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## &#x1F531; Step 8: Define Graph Components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we define a class and two functions. The class `RAGState` outlines the structure that each node of our langgraph graph will follow - there will always be a question, and we might have some helpful documents and eventually an answer.\n",
    "\n",
    "The functions describe what will happen in two nodes of the graph:\n",
    "* In a `retrieve` node, we'll get the current question from the state, search for documents similar to the question in the vectorstore, and then update the state so that docs points to the relevant documents we found.\n",
    "* In a `generate` node, we'll get the current question and docs from the state, construct a prompt so that it contains the relevant documents that were found, the question, and a slot for the answer, then pass the prompt to our language model. It then updates the state so that answer contains the response from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGState(TypedDict):\n",
    "    question: str\n",
    "    docs: Optional[List[Document]]\n",
    "    answer: Optional[str]\n",
    "\n",
    "def retrieve(state: RAGState) -> RAGState:\n",
    "    question = state[\"question\"]\n",
    "    results = retriever.invoke(question)\n",
    "    return {\"question\": question, \"docs\": results, \"answer\": None}\n",
    "\n",
    "def generate(state: RAGState) -> RAGState:\n",
    "    question = state[\"question\"]\n",
    "    docs = state[\"docs\"] or []\n",
    "    context = \"\\n\".join([doc.page_content for doc in docs])\n",
    "    prompt_template = f\"\"\"<|system|>\n",
    "    Context: {context}\n",
    "    Given the context, write an answer to the question. \n",
    "    Only provide an answer, do not detail the context or question.<|end|>\n",
    "    <|user|>\n",
    "    {question}<|end|>\n",
    "    <|assistant|>\"\"\"\n",
    "    prompt = prompt_template\n",
    "    result = llm.invoke(prompt)\n",
    "    return {\"question\": question, \"docs\": docs, \"answer\": result}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## &#x25B6; Step 9: Build Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we construct our graph. The flow of events will be retrieve -> generate -> answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "builder = StateGraph(RAGState)\n",
    "builder.add_node(\"retrieve\", RunnableLambda(retrieve))\n",
    "builder.add_node(\"generate\", RunnableLambda(generate))\n",
    "builder.set_entry_point(\"retrieve\")\n",
    "builder.add_edge(\"retrieve\", \"generate\")\n",
    "builder.add_edge(\"generate\", END)\n",
    "graph = builder.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WnhqZpXv1eur"
   },
   "source": [
    "## 🧪 Step 10: Test the System by Asking a Question it Should Know About"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's quickly test how the model does without the added context. The specific answer we are looking for is \"Distributed Adaptive Message Block Switching\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "question = \"What did Paul Baran develop\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ragless_prompt = f\"\"\"<|system|>\n",
    "    Write a concise answer to the question or explain that you don't know it.<|end|>\n",
    "    <|user|>\n",
    "    {question}<|end|>\n",
    "    <|assistant|>\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ragless_result = llm.invoke(ragless_prompt)\n",
    "print(\"Answer:\", ragless_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see how the model does with the added RAG context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result = graph.invoke({\"question\": question, \"docs\": None, \"answer\": None})\n",
    "print(\"Answer:\", result[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we can see the 3 documents that were found to give context to the model. From this, it is able to answer our query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result['docs']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ✅❌ Step 11: Evaluate System Performance\n",
    "Here we have a small piece of code which we can use to evaluate the performance of our RAG system. It takes the given answer to a given question and checks to see if it is in the response generated by the system. We do that 5 times and find the percentage of the time we are correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re, random\n",
    "\n",
    "def normalize(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    return re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "random.seed(1)\n",
    "test_indices = [random.randint(0, 10_000) for _ in range(5)]\n",
    "for i in test_indices:\n",
    "    question = squad[i][\"question\"]\n",
    "    ground_truth = squad[i][\"answers\"][\"text\"][0] if squad[i][\"answers\"][\"text\"] else \"\"\n",
    "    response = graph.invoke({\"question\": question})\n",
    "    prediction = response[\"answer\"]\n",
    "    norm_pred = normalize(prediction)\n",
    "    norm_truth = normalize(ground_truth)\n",
    "\n",
    "    if norm_truth in norm_pred or norm_pred in norm_truth:\n",
    "        correct += 1\n",
    "    total += 1\n",
    "\n",
    "    print(f\"\\nQ: {question}\\nPredicted: {prediction}\\nActual: {ground_truth}\")\n",
    "print(f\"\\nAccuracy: {correct}/{total} = {correct / total:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "13bf6404",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Extension  🌟\n",
    "* Select a question from the SQuAD dataset and see how well you think the system answers it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
